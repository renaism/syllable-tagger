{
 "nbformat": 4,
 "nbformat_minor": 2,
 "metadata": {
  "language_info": {
   "name": "python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "version": "3.7.4-final"
  },
  "orig_nbformat": 2,
  "file_extension": ".py",
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 },
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import time\n",
    "import src.ngram as ngram\n",
    "from src.training.preprocess import tokenize, pad_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train_folds_fnames = {\n",
    "    1: '../data/trainset/train_fold_1.txt',\n",
    "    2: '../data/trainset/train_fold_2.txt',\n",
    "    3: '../data/trainset/train_fold_3.txt',\n",
    "    4: '../data/trainset/train_fold_4.txt',\n",
    "    5: '../data/trainset/train_fold_5.txt'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aug_train_fold_fnames = {\n",
    "    1: '../data/trainset/augmented/aug_train_fold_1.txt',\n",
    "    2: '../data/trainset/augmented/aug_train_fold_2.txt',\n",
    "    3: '../data/trainset/augmented/aug_train_fold_3.txt',\n",
    "    4: '../data/trainset/augmented/aug_train_fold_4.txt',\n",
    "    5: '../data/trainset/augmented/aug_train_fold_5.txt'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_max = 5\n",
    "k_min = 1\n",
    "k_max = 5\n",
    "train_set_type = 'augmented' # Set to original / augmented\n",
    "\n",
    "start_t = time.time()\n",
    "\n",
    "print('Max n: ', n_max)\n",
    "print('Train set: ', train_set_type)\n",
    "\n",
    "if train_set_type == 'original':\n",
    "    folds_fnames = data_train_folds_fnames\n",
    "    output_fname_suffix = '../models/ngrams/{}_gram_'.format(n_max)\n",
    "elif train_set_type == 'augmented':\n",
    "    folds_fnames = aug_train_fold_fnames\n",
    "    output_fname_suffix = '../models/ngrams/{}_gram_aug_'.format(n_max)\n",
    "\n",
    "for fold in range(k_min, k_max+1):\n",
    "    print('\\nFold {}/{}'.format(fold, k_max))\n",
    "    \n",
    "    # Load train set\n",
    "    fname = folds_fnames[fold]\n",
    "    print('Train set: ', fname)\n",
    "    data_train = pd.read_csv(\n",
    "        fname, \n",
    "        sep='\\t', \n",
    "        header=None, \n",
    "        names=['word', 'syllables'], \n",
    "        na_filter=False\n",
    "    )\n",
    "    print('Number of words: ', len(data_train))\n",
    "\n",
    "    # Build the n-gram\n",
    "    print('Building n-gram')\n",
    "    tokens = pad_tokens(tokenize(data_train), n=n_max, start_pad=True, end_marker=True)\n",
    "    ngram_fold = ngram.NGram(tokens, n=n_max, verbose=True)\n",
    "\n",
    "    # Save the n-gram to a file\n",
    "    fname = output_fname_suffix + 'fold_{}.json'.format(fold)\n",
    "    ngram.save(ngram_fold, fname)\n",
    "    print('n-gram saved to \"{}\"'.format(fname))\n",
    "\n",
    "print('\\nAll n-grams generated in {:.2f} s'.format(time.time() - start_t))"
   ]
  }
 ]
}